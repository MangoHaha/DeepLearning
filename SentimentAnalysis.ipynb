{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoHaha/DeepLearning/blob/investigate_new_Arch/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1",
        "id": "CDx4CA09GZG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## General information\n",
        "\n",
        "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
        "\n",
        "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
      ]
    },
    {
      "metadata": {
        "id": "Vq4S5_HIGpBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "bf293e78-11bd-4b8e-8c3f-f407ad7514c8"
      },
      "cell_type": "code",
      "source": [
        "!pip install lightgbm wordcloud\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightgbm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/7e/bc87e7951cfaa998cffaf39e6c721f5bd04efb2e139486206356edb289a5/lightgbm-2.2.1-py2.py3-none-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.1MB 10.0MB/s \n",
            "\u001b[?25hCollecting wordcloud\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/af/849edf14d573eba9c8082db898ff0d090428d9485371cc4fe21a66717ad2/wordcloud-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (361kB)\n",
            "\u001b[K    100% |████████████████████████████████| 368kB 30.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.46)\n",
            "Installing collected packages: lightgbm, wordcloud\n",
            "Successfully installed lightgbm-2.2.1 wordcloud-1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "9K7leB0DGZG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "69503912-0a32-4313-b13b-395b6a2e3d4b"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import datetime\n",
        "import lightgbm as lgb\n",
        "from scipy import stats\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "pd.set_option('max_colwidth',400)\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from google.colab import files\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NOvuWhQPLPmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "197d8a0f-a741-42ed-d583-9599dd9af3f5"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A33LQkFPNeDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "992bf410-e3fe-4e96-b643-39d60b879f04"
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/DeepLearning\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crawl-300d-2M.vec\t   sampleSubmission.csv  train.tsv\n",
            "glove.twitter.27B.25d.txt  test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true,
        "id": "Qp7EQ0TrGZHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train = pd.read_csv('/content/drive/My Drive/DeepLearning/train.tsv', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/My Drive/DeepLearning/test.tsv', sep=\"\\t\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c",
        "id": "xBg-49HYGZHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78",
        "id": "AE2gjhWFGZHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.loc[train.SentenceId == 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1",
        "id": "WUMsIz3fGZHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
        "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e",
        "id": "82tj6yniGZHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
        "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128",
        "id": "jlqd1ZRtGZHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
        "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2",
        "id": "hWLkVYcCGZHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
        "collapsed": true,
        "id": "LNCjPzRnGZHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see for example most common trigrams for positive phrases"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10",
        "id": "4khms7mwGZHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
        "text_trigrams = [i for i in ngrams(text.split(), 3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119",
        "id": "WiF4y7O_GZHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Counter(text_trigrams).most_common(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55",
        "id": "zr1gESsOGZHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "7c744e11-35e8-4d7c-90b7-256dae10b227"
      },
      "cell_type": "code",
      "source": [
        "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
        "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
        "text_trigrams = [i for i in ngrams(text, 3)]\n",
        "Counter(text_trigrams).most_common(30)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', 'funny', ','), 33),\n",
              " (('one', 'year', \"'s\"), 28),\n",
              " (('year', \"'s\", 'best'), 26),\n",
              " (('movies', 'ever', 'made'), 19),\n",
              " ((',', 'solid', 'cast'), 19),\n",
              " (('solid', 'cast', ','), 18),\n",
              " ((\"'ve\", 'ever', 'seen'), 16),\n",
              " (('.', 'It', \"'s\"), 16),\n",
              " ((',', 'making', 'one'), 15),\n",
              " (('best', 'films', 'year'), 15),\n",
              " ((',', 'touching', ','), 15),\n",
              " (('exquisite', 'acting', ','), 15),\n",
              " (('acting', ',', 'inventive'), 14),\n",
              " ((',', 'inventive', 'screenplay'), 14),\n",
              " (('jaw-dropping', 'action', 'sequences'), 14),\n",
              " (('good', 'acting', ','), 14),\n",
              " ((\"'s\", 'best', 'films'), 14),\n",
              " (('I', \"'ve\", 'seen'), 14),\n",
              " (('funny', ',', 'even'), 14),\n",
              " (('best', 'war', 'movies'), 13),\n",
              " (('purely', 'enjoyable', 'satisfying'), 13),\n",
              " (('funny', ',', 'touching'), 13),\n",
              " ((',', 'smart', ','), 13),\n",
              " (('inventive', 'screenplay', ','), 13),\n",
              " (('funniest', 'jokes', 'movie'), 13),\n",
              " (('action', 'sequences', ','), 13),\n",
              " (('sequences', ',', 'striking'), 13),\n",
              " ((',', 'striking', 'villains'), 13),\n",
              " (('exquisite', 'motion', 'picture'), 13),\n",
              " (('war', 'movies', 'ever'), 12)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74",
        "id": "KZBtEbGKGZHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text."
      ]
    },
    {
      "metadata": {
        "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee",
        "id": "4qTvvMwYGZHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Thoughts on feature processing and engineering"
      ]
    },
    {
      "metadata": {
        "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b",
        "id": "lUzx6-B0GZHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
        "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
        "- puntuation could be important, so it should be used;\n",
        "- ngrams are necessary to get the most info from data;\n",
        "- using features like word count or sentence length won't be useful;"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5",
        "id": "XEQTtoabGZHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d",
        "id": "nbSy71IgGZHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
        "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
        "vectorizer.fit(full_text)\n",
        "train_vectorized = vectorizer.transform(train['Phrase'])\n",
        "test_vectorized = vectorizer.transform(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZadwpFibkWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d3eecbf-c0b0-475d-f9df-1baf7d0837ea"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(vectorizer.get_feature_names()) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60",
        "id": "Wq4b0ssAGZH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8",
        "id": "8-9VdgD0GZH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "ovr = OneVsRestClassifier(logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82",
        "id": "RGIQrR6GGZH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ovr.fit(train_vectorized, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe",
        "id": "EM7TTGtFGZH8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c",
        "id": "4mA7KtH8GZIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "svc = LinearSVC(dual=False)\n",
        "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184",
        "id": "2ZLraC0oGZID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ovr.fit(train_vectorized, y);\n",
        "svc.fit(train_vectorized, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1",
        "id": "D-qyZxyVGZIG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep learning\n",
        "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618",
        "id": "AwnzDD_0GZIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbea5c28-b6e3-46d8-c2f9-c38806b94ab0"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80",
        "id": "TP2z0XJhGZIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(lower = True, filters='')\n",
        "tk.fit_on_texts(full_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f",
        "id": "10YBW01vGZIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
        "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b",
        "id": "b6bErvirGZIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c",
        "id": "x2VpwGsgGZIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = \"/content/drive/My Drive/DeepLearning/crawl-300d-2M.vec\"\n",
        "#embedding_path = \"/content/drive/My Drive/DeepLearning/glove.twitter.27B.25d.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd",
        "id": "pXkIUeQ5GZIU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "#embed_size = 25\n",
        "max_features = 30000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1",
        "id": "oiQRmxWuGZIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
        "\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855",
        "id": "4AlRADppGZIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47",
        "id": "fdY5hCa9GZIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An attempt at ensemble:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe",
        "id": "jUBolZU1GZIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    \n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    \n",
        "    \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441",
        "id": "sCmRToDbGZIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "54f0ddf1-b0ab-451b-f0f7-64f15c19e2ed"
      },
      "cell_type": "code",
      "source": [
        "model1 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140454 samples, validate on 15606 samples\n",
            "Epoch 1/20\n",
            "140454/140454 [==============================] - 81s 578us/step - loss: 0.3637 - acc: 0.8385 - val_loss: 0.3267 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.32669, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 2/20\n",
            "140454/140454 [==============================] - 70s 501us/step - loss: 0.3248 - acc: 0.8538 - val_loss: 0.3136 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.32669 to 0.31362, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 3/20\n",
            "140454/140454 [==============================] - 71s 507us/step - loss: 0.3141 - acc: 0.8571 - val_loss: 0.3060 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31362 to 0.30603, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 4/20\n",
            "140454/140454 [==============================] - 70s 497us/step - loss: 0.3056 - acc: 0.8607 - val_loss: 0.3079 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.30603\n",
            "Epoch 5/20\n",
            "140454/140454 [==============================] - 69s 492us/step - loss: 0.2999 - acc: 0.8631 - val_loss: 0.3041 - val_acc: 0.8587\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.30603 to 0.30413, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 6/20\n",
            "140454/140454 [==============================] - 69s 494us/step - loss: 0.2950 - acc: 0.8657 - val_loss: 0.3014 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.30413 to 0.30140, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 7/20\n",
            "140454/140454 [==============================] - 70s 500us/step - loss: 0.2899 - acc: 0.8675 - val_loss: 0.2999 - val_acc: 0.8618\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.30140 to 0.29989, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 8/20\n",
            "140454/140454 [==============================] - 68s 486us/step - loss: 0.2877 - acc: 0.8687 - val_loss: 0.2996 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.29989 to 0.29962, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 9/20\n",
            "140454/140454 [==============================] - 71s 503us/step - loss: 0.2838 - acc: 0.8705 - val_loss: 0.3001 - val_acc: 0.8617\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.29962\n",
            "Epoch 10/20\n",
            "140454/140454 [==============================] - 70s 496us/step - loss: 0.2817 - acc: 0.8715 - val_loss: 0.2979 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.29962 to 0.29788, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 11/20\n",
            "140454/140454 [==============================] - 68s 481us/step - loss: 0.2792 - acc: 0.8727 - val_loss: 0.3009 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.29788\n",
            "Epoch 12/20\n",
            "140454/140454 [==============================] - 68s 485us/step - loss: 0.2762 - acc: 0.8746 - val_loss: 0.2971 - val_acc: 0.8627\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.29788 to 0.29714, saving model to /content/drive/My Drive/DeepLearning/best_model.hdf5\n",
            "Epoch 13/20\n",
            "140454/140454 [==============================] - 68s 482us/step - loss: 0.2750 - acc: 0.8748 - val_loss: 0.3017 - val_acc: 0.8630\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29714\n",
            "Epoch 14/20\n",
            "140454/140454 [==============================] - 68s 481us/step - loss: 0.2732 - acc: 0.8760 - val_loss: 0.3031 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29714\n",
            "Epoch 15/20\n",
            "140454/140454 [==============================] - 68s 482us/step - loss: 0.2714 - acc: 0.8769 - val_loss: 0.3061 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.29714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b10113439be683bf19930750eaf96328e5b58d42",
        "id": "45uzbiL4GZIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OprQPJHYgWIy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(128270, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)       \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    x = max_pool1_lstm\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJ9oIJWPgna1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model3 = build_model3(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966",
        "id": "xu_ILbwzGZIu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred = pred1\n",
        "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred2\n",
        "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971",
        "id": "_GvpDKB1GZIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/drive/My Drive/DeepLearning/blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}