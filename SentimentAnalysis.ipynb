{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoHaha/SentimentAnalysis/blob/master/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1",
        "id": "CDx4CA09GZG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## General information\n",
        "\n",
        "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n"
      ]
    },
    {
      "metadata": {
        "id": "Vq4S5_HIGpBc",
        "colab_type": "code",
        "outputId": "ee44f161-e1cf-481d-8781-5b9e53a70282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3400
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install lightgbm wordcloud\n",
        "!pip install pydot && apt-get install graphviz\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lightgbm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/3b/4ae113193b4ee01387ed76d5eea32788aec0589df9ae7378a8b7443eaa8b/lightgbm-2.2.2-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 10.6MB/s \n",
            "\u001b[?25hCollecting wordcloud\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/af/849edf14d573eba9c8082db898ff0d090428d9485371cc4fe21a66717ad2/wordcloud-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (361kB)\n",
            "\u001b[K    100% |████████████████████████████████| 368kB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.46)\n",
            "Installing collected packages: lightgbm, wordcloud\n",
            "Successfully installed lightgbm-2.2.2 wordcloud-1.5.0\n",
            "Collecting pydot\n",
            "  Downloading https://files.pythonhosted.org/packages/53/11/9db5c788f5ad05438b7c2a07fd7edd9820b7f3d95bb0690a16f7bf426204/pydot-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.3.0)\n",
            "Installing collected packages: pydot\n",
            "Successfully installed pydot-1.4.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fontconfig libann0 libcairo2 libcdt5 libcgraph6 libdatrie1 libgd3\n",
            "  libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libjbig0 liblab-gamut1 libltdl7\n",
            "  libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
            "  libpixman-1-0 libthai-data libthai0 libtiff5 libwebp6 libxaw7 libxcb-render0\n",
            "  libxcb-shm0 libxmu6 libxpm4 libxt6\n",
            "Suggested packages:\n",
            "  gsfonts graphviz-doc libgd-tools\n",
            "The following NEW packages will be installed:\n",
            "  fontconfig graphviz libann0 libcairo2 libcdt5 libcgraph6 libdatrie1 libgd3\n",
            "  libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libjbig0 liblab-gamut1 libltdl7\n",
            "  libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
            "  libpixman-1-0 libthai-data libthai0 libtiff5 libwebp6 libxaw7 libxcb-render0\n",
            "  libxcb-shm0 libxmu6 libxpm4 libxt6\n",
            "0 upgraded, 30 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 4,154 kB of archives.\n",
            "After this operation, 16.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fontconfig amd64 2.12.6-0ubuntu2 [169 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libann0 amd64 1.1.2+doc-6 [24.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcdt5 amd64 2.40.1-2 [19.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcgraph6 amd64 2.40.1-2 [40.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig0 amd64 2.1-3.1build1 [26.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtiff5 amd64 4.0.9-5 [152 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwebp6 amd64 0.6.1-2 [185 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxpm4 amd64 1:3.5.12-1 [34.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgd3 amd64 2.2.5-4ubuntu0.2 [119 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-0 amd64 0.34.0-2 [229 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcb-render0 amd64 1.13-1 [14.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcb-shm0 amd64 1.13-1 [5,572 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcairo2 amd64 1.15.10-2 [580 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libthai-data all 0.1.27-2 [133 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdatrie1 amd64 0.2.10-7 [17.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libthai0 amd64 0.1.27-2 [18.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango-1.0-0 amd64 1.40.14-1ubuntu0.1 [153 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoft2-1.0-0 amd64 1.40.14-1ubuntu0.1 [33.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangocairo-1.0-0 amd64 1.40.14-1ubuntu0.1 [20.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpathplan4 amd64 2.40.1-2 [22.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvc6 amd64 2.40.1-2 [601 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvpr2 amd64 2.40.1-2 [169 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblab-gamut1 amd64 2.40.1-2 [178 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxt6 amd64 1:1.1.5-1 [160 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmu6 amd64 2:1.1.2-2 [46.0 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxaw7 amd64 2:1.0.13-1 [173 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 graphviz amd64 2.40.1-2 [601 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\n",
            "Fetched 4,154 kB in 2s (2,274 kB/s)\n",
            "Selecting previously unselected package fontconfig.\n",
            "(Reading database ... 26397 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fontconfig_2.12.6-0ubuntu2_amd64.deb ...\n",
            "Unpacking fontconfig (2.12.6-0ubuntu2) ...\n",
            "Selecting previously unselected package libann0.\n",
            "Preparing to unpack .../01-libann0_1.1.2+doc-6_amd64.deb ...\n",
            "Unpacking libann0 (1.1.2+doc-6) ...\n",
            "Selecting previously unselected package libcdt5.\n",
            "Preparing to unpack .../02-libcdt5_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcdt5 (2.40.1-2) ...\n",
            "Selecting previously unselected package libcgraph6.\n",
            "Preparing to unpack .../03-libcgraph6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcgraph6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "Preparing to unpack .../04-libjbig0_2.1-3.1build1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../05-libtiff5_4.0.9-5_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.9-5) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../06-libwebp6_0.6.1-2_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.1-2) ...\n",
            "Selecting previously unselected package libxpm4:amd64.\n",
            "Preparing to unpack .../07-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libgd3:amd64.\n",
            "Preparing to unpack .../08-libgd3_2.2.5-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Selecting previously unselected package libgts-0.7-5:amd64.\n",
            "Preparing to unpack .../09-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Selecting previously unselected package libpixman-1-0:amd64.\n",
            "Preparing to unpack .../10-libpixman-1-0_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libxcb-render0:amd64.\n",
            "Preparing to unpack .../11-libxcb-render0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-render0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libxcb-shm0:amd64.\n",
            "Preparing to unpack .../12-libxcb-shm0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-shm0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libcairo2:amd64.\n",
            "Preparing to unpack .../13-libcairo2_1.15.10-2_amd64.deb ...\n",
            "Unpacking libcairo2:amd64 (1.15.10-2) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../14-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libthai-data.\n",
            "Preparing to unpack .../15-libthai-data_0.1.27-2_all.deb ...\n",
            "Unpacking libthai-data (0.1.27-2) ...\n",
            "Selecting previously unselected package libdatrie1:amd64.\n",
            "Preparing to unpack .../16-libdatrie1_0.2.10-7_amd64.deb ...\n",
            "Unpacking libdatrie1:amd64 (0.2.10-7) ...\n",
            "Selecting previously unselected package libthai0:amd64.\n",
            "Preparing to unpack .../17-libthai0_0.1.27-2_amd64.deb ...\n",
            "Unpacking libthai0:amd64 (0.1.27-2) ...\n",
            "Selecting previously unselected package libpango-1.0-0:amd64.\n",
            "Preparing to unpack .../18-libpango-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
            "Preparing to unpack .../19-libpangoft2-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
            "Preparing to unpack .../20-libpangocairo-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpathplan4.\n",
            "Preparing to unpack .../21-libpathplan4_2.40.1-2_amd64.deb ...\n",
            "Unpacking libpathplan4 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../22-libgvc6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvc6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvpr2.\n",
            "Preparing to unpack .../23-libgvpr2_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvpr2 (2.40.1-2) ...\n",
            "Selecting previously unselected package liblab-gamut1.\n",
            "Preparing to unpack .../24-liblab-gamut1_2.40.1-2_amd64.deb ...\n",
            "Unpacking liblab-gamut1 (2.40.1-2) ...\n",
            "Selecting previously unselected package libxt6:amd64.\n",
            "Preparing to unpack .../25-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
            "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
            "Selecting previously unselected package libxmu6:amd64.\n",
            "Preparing to unpack .../26-libxmu6_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxaw7:amd64.\n",
            "Preparing to unpack .../27-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../28-graphviz_2.40.1-2_amd64.deb ...\n",
            "Unpacking graphviz (2.40.1-2) ...\n",
            "Selecting previously unselected package libgts-bin.\n",
            "Preparing to unpack .../29-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Setting up libpathplan4 (2.40.1-2) ...\n",
            "Setting up liblab-gamut1 (2.40.1-2) ...\n",
            "Setting up libxcb-render0:amd64 (1.13-1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Setting up libdatrie1:amd64 (0.2.10-7) ...\n",
            "Setting up libtiff5:amd64 (4.0.9-5) ...\n",
            "Setting up libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libann0 (1.1.2+doc-6) ...\n",
            "Setting up libxcb-shm0:amd64 (1.13-1) ...\n",
            "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Setting up libxt6:amd64 (1:1.1.5-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libthai-data (0.1.27-2) ...\n",
            "Setting up libcdt5 (2.40.1-2) ...\n",
            "Setting up fontconfig (2.12.6-0ubuntu2) ...\n",
            "Regenerating fonts cache... done.\n",
            "Setting up libcgraph6 (2.40.1-2) ...\n",
            "Setting up libwebp6:amd64 (0.6.1-2) ...\n",
            "Setting up libcairo2:amd64 (1.15.10-2) ...\n",
            "Setting up libgvpr2 (2.40.1-2) ...\n",
            "Setting up libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Setting up libthai0:amd64 (0.1.27-2) ...\n",
            "Setting up libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Setting up libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Setting up libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libgvc6 (2.40.1-2) ...\n",
            "Setting up graphviz (2.40.1-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "9K7leB0DGZG9",
        "colab_type": "code",
        "outputId": "41e1395c-bd24-4b57-9404-c81056e458c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import datetime\n",
        "import lightgbm as lgb\n",
        "from scipy import stats\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "pd.set_option('max_colwidth',400)\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yoOIXlOaxB9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preparation:\n",
        "Import dataset are stored inside my google drive"
      ]
    },
    {
      "metadata": {
        "id": "NOvuWhQPLPmH",
        "colab_type": "code",
        "outputId": "41ae6204-f002-4445-bcf5-b4281f02ced6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZnmH5hSQxQYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read dataset: train test submission"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true,
        "id": "Qp7EQ0TrGZHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/DeepLearning/train.tsv', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/My Drive/DeepLearning/test.tsv', sep=\"\\t\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "y = train['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rilTf7HZbM4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging.\n",
        "We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c",
        "id": "xBg-49HYGZHE",
        "colab_type": "code",
        "outputId": "ae41b3cc-4dd8-4551-9a0c-75dbf3e5ab8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "#print(train.head(10))\n",
        "print(train.loc[train.SentenceId == 20])\n",
        "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
        "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))\n",
        "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
        "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))\n",
        "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
        "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     PhraseId  SentenceId  \\\n",
            "536       537          20   \n",
            "537       538          20   \n",
            "538       539          20   \n",
            "539       540          20   \n",
            "540       541          20   \n",
            "541       542          20   \n",
            "542       543          20   \n",
            "543       544          20   \n",
            "544       545          20   \n",
            "545       546          20   \n",
            "546       547          20   \n",
            "547       548          20   \n",
            "548       549          20   \n",
            "549       550          20   \n",
            "550       551          20   \n",
            "\n",
            "                                                   Phrase  Sentiment  \n",
            "536  It 's everything you 'd expect -- but nothing more .          2  \n",
            "537     's everything you 'd expect -- but nothing more .          1  \n",
            "538       's everything you 'd expect -- but nothing more          2  \n",
            "539          everything you 'd expect -- but nothing more          1  \n",
            "540                                            everything          2  \n",
            "541                     you 'd expect -- but nothing more          1  \n",
            "542                         'd expect -- but nothing more          2  \n",
            "543                                                    'd          2  \n",
            "544                            expect -- but nothing more          2  \n",
            "545                                 expect -- but nothing          2  \n",
            "546                                             expect --          2  \n",
            "547                                                expect          2  \n",
            "548                                           but nothing          2  \n",
            "549                                               nothing          1  \n",
            "550                                                  more          2  \n",
            "Average count of phrases per sentence in train is 18.\n",
            "Average count of phrases per sentence in test is 20.\n",
            "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
            "Number of phrases in test: 66292. Number of sentences in test: 3310.\n",
            "Average word length of phrases in train is 7.\n",
            "Average word length of phrases in test is 7.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tI8pN6fob_mC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On Overlapping sentences in both Train and Test set"
      ]
    },
    {
      "metadata": {
        "id": "7K_b0M-0Eye9",
        "colab_type": "code",
        "outputId": "e0d12d5a-349e-4648-ab49-0126bbfe045d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "save_test = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"inner\")\n",
        "print (\"Number of overlapping phrases  \", save_test.shape[0])\n",
        "print (\"% of neutral sentiment phrases\",save_test[(save_test['Sentiment'] == 2)].count()[0] /save_test.shape[0])\n",
        "save_test = save_test[save_test[\"Sentiment\"].notnull()]\n",
        "save_test.drop(['SentenceId', 'Phrase'], axis=1,inplace=True)\n",
        "save_test = save_test[save_test[\"Sentiment\"].notnull()]\n",
        "\n",
        "import math\n",
        "def get_sentiment(row):\n",
        "    old_s = row['Sentiment_x']\n",
        "    new_s = row['Sentiment_y']\n",
        "    if math.isnan(new_s):\n",
        "        return int(old_s)\n",
        "    else:\n",
        "        return int(new_s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of overlapping phrases   10297\n",
            "% of neutral sentiment phrases 0.7301155676410604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1",
        "id": "D-qyZxyVGZIG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618",
        "id": "AwnzDD_0GZIH",
        "colab_type": "code",
        "outputId": "a520e46e-1794-4ea8-ba39-d9ce0ea3c23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, LeakyReLU\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6IRBrjIkWDq8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Embedding & Word Vectorization"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80",
        "id": "TP2z0XJhGZIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(lower = True, filters='')\n",
        "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
        "tk.fit_on_texts(full_text)\n",
        "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
        "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbNFsH5cHoiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pre-trained word2vec:  this model is trained on thecontext on each word so that similar words will havesimilar numerical representations. Sentences are firsttokenized to create a number of pairs of words, de-pending on the window size.  Then the data it’s fedinto  a  neural  network  through  an  embedding  layerinitialized with random weights.  Once the model istrained to minimize the loss of predicting the targetwords  using  the  context  words,  the  weights  in  theembedding layer would represent the vocabulary ofword vectors"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c",
        "id": "x2VpwGsgGZIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = \"/content/drive/My Drive/DeepLearning/crawl-300d-2M.vec\"\n",
        "#embedding_path = \"/content/drive/My Drive/DeepLearning/glove.twitter.27B.25d.txt\"\n",
        "#embed_size = 25\n",
        "embed_size = 300\n",
        "max_features = 30000\n",
        "\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
        "\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b",
        "id": "b6bErvirGZIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855",
        "id": "4AlRADppGZIa",
        "colab_type": "code",
        "outputId": "7b7b8a7e-8822-418f-ed21-05f49b94a63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "ohe.fit(train[\"Sentiment\"].values.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
              "       handle_unknown='error', n_values='auto', sparse=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "x2cpGj5WcMsH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Model 1: Embedding + {LSTM/GRU} + CNN"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe",
        "id": "jUBolZU1GZIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    \n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    \n",
        "    \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wjh0IK94gE7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print the Summary and Arch of the model"
      ]
    },
    {
      "metadata": {
        "id": "yrvsOTvmOPvZ",
        "colab_type": "code",
        "outputId": "2cc4f555-8a55-483e-ebd2-3a41fb6cbd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        }
      },
      "cell_type": "code",
      "source": [
        "model1 = build_model1(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)\n",
        "model1.summary()\n",
        "SVG(model_to_dot(trained_model1,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 50, 300)      5843700     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 50, 300)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 50, 128)      140544      spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 50, 128)      187392      spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 48, 32)       12320       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 48, 32)       12320       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 48, 32)       12320       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 48, 32)       12320       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 32)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 32)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 32)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 32)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 32)           0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 32)           0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_4 (Glo (None, 32)           0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_4 (GlobalM (None, 32)           0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_average_pooling1d_3[0][0] \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "                                                                 global_average_pooling1d_4[0][0] \n",
            "                                                                 global_max_pooling1d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256)          1024        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           16448       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64)           256         dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           2080        batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 5)            165         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,240,889\n",
            "Trainable params: 396,549\n",
            "Non-trainable params: 5,844,340\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q7cn0Rl5R6KJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "K folds Validation"
      ]
    },
    {
      "metadata": {
        "id": "O9OLbhyJTfMc",
        "colab_type": "code",
        "outputId": "6ee8bd27-0d55-4636-c033-93374d3602a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1479
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_FOLDS = 5\n",
        "train[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n",
        "test_preds = np.zeros((test.shape[0], 5))\n",
        "file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                              save_best_only = True, mode = \"min\")\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 2)\n",
        "\n",
        "print(\"Building the model...\")\n",
        "model1 = build_model1(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    print(\"FOLD\", i+1)    \n",
        "    print(\"Splitting the data into train and validation...\")\n",
        "    train_seq, val_seq = X_train[train[\"fold_id\"] != i], X_train[train[\"fold_id\"] == i]\n",
        "    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
        "    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))        \n",
        "    \n",
        "    print(\"Training the model...\")\n",
        "    model1.fit(train_seq, y_train, validation_data = (val_seq, y_val), batch_size = 128, epochs = 15, verbose = 1, callbacks = [early_stop]) \n",
        "    model1.save_weights(file_path)  \n",
        "    test_preds += model1.predict([X_test], batch_size=1024, verbose=1)    \n",
        "    print()\n",
        "    \n",
        "print(\"Save model after cross-validation...\")\n",
        "model1.save_weights(file_path)   \n",
        "test_preds /= NUM_FOLDS\n",
        "\n",
        "\n",
        "print(\"Make the submission ready...\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "\n",
        "pred = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/blend.csv\", index=False)\n",
        "\n",
        "predictions = np.round(np.argmax(test_preds, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/avg_blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the model...\n",
            "FOLD 1\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 125230 samples, validate on 30830 samples\n",
            "Epoch 1/15\n",
            "125230/125230 [==============================] - 70s 557us/step - loss: 0.3728 - acc: 0.8319 - val_loss: 0.3122 - val_acc: 0.8580\n",
            "Epoch 2/15\n",
            "125230/125230 [==============================] - 62s 493us/step - loss: 0.3281 - acc: 0.8515 - val_loss: 0.3031 - val_acc: 0.8599\n",
            "Epoch 3/15\n",
            "125230/125230 [==============================] - 62s 492us/step - loss: 0.3184 - acc: 0.8549 - val_loss: 0.3007 - val_acc: 0.8629\n",
            "Epoch 4/15\n",
            "125230/125230 [==============================] - 62s 494us/step - loss: 0.3115 - acc: 0.8572 - val_loss: 0.2998 - val_acc: 0.8620\n",
            "Epoch 5/15\n",
            "125230/125230 [==============================] - 62s 493us/step - loss: 0.3046 - acc: 0.8599 - val_loss: 0.2952 - val_acc: 0.8663\n",
            "Epoch 6/15\n",
            "125230/125230 [==============================] - 62s 493us/step - loss: 0.2976 - acc: 0.8632 - val_loss: 0.2900 - val_acc: 0.8669\n",
            "Epoch 7/15\n",
            "125230/125230 [==============================] - 61s 489us/step - loss: 0.2941 - acc: 0.8648 - val_loss: 0.2888 - val_acc: 0.8678\n",
            "Epoch 8/15\n",
            "125230/125230 [==============================] - 61s 491us/step - loss: 0.2897 - acc: 0.8668 - val_loss: 0.2901 - val_acc: 0.8676\n",
            "Epoch 9/15\n",
            "125230/125230 [==============================] - 61s 490us/step - loss: 0.2868 - acc: 0.8683 - val_loss: 0.2883 - val_acc: 0.8687\n",
            "Epoch 10/15\n",
            "125230/125230 [==============================] - 61s 489us/step - loss: 0.2835 - acc: 0.8698 - val_loss: 0.2893 - val_acc: 0.8686\n",
            "Epoch 11/15\n",
            "125230/125230 [==============================] - 61s 490us/step - loss: 0.2812 - acc: 0.8710 - val_loss: 0.2877 - val_acc: 0.8688\n",
            "Epoch 12/15\n",
            "125230/125230 [==============================] - 61s 490us/step - loss: 0.2783 - acc: 0.8726 - val_loss: 0.2865 - val_acc: 0.8691\n",
            "Epoch 13/15\n",
            "125230/125230 [==============================] - 61s 490us/step - loss: 0.2762 - acc: 0.8739 - val_loss: 0.2898 - val_acc: 0.8666\n",
            "Epoch 14/15\n",
            "125230/125230 [==============================] - 61s 491us/step - loss: 0.2745 - acc: 0.8749 - val_loss: 0.2872 - val_acc: 0.8696\n",
            "66292/66292 [==============================] - 4s 64us/step\n",
            "\n",
            "FOLD 2\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124608 samples, validate on 31452 samples\n",
            "Epoch 1/15\n",
            "124608/124608 [==============================] - 61s 488us/step - loss: 0.2808 - acc: 0.8720 - val_loss: 0.2489 - val_acc: 0.8887\n",
            "Epoch 2/15\n",
            "124608/124608 [==============================] - 61s 491us/step - loss: 0.2782 - acc: 0.8728 - val_loss: 0.2524 - val_acc: 0.8877\n",
            "Epoch 3/15\n",
            "124608/124608 [==============================] - 61s 490us/step - loss: 0.2751 - acc: 0.8746 - val_loss: 0.2513 - val_acc: 0.8877\n",
            "66292/66292 [==============================] - 3s 48us/step\n",
            "\n",
            "FOLD 3\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 125022 samples, validate on 31038 samples\n",
            "Epoch 1/15\n",
            "125022/125022 [==============================] - 61s 488us/step - loss: 0.2765 - acc: 0.8740 - val_loss: 0.2355 - val_acc: 0.8952\n",
            "Epoch 2/15\n",
            "125022/125022 [==============================] - 61s 490us/step - loss: 0.2742 - acc: 0.8755 - val_loss: 0.2378 - val_acc: 0.8928\n",
            "Epoch 3/15\n",
            "125022/125022 [==============================] - 61s 489us/step - loss: 0.2727 - acc: 0.8764 - val_loss: 0.2400 - val_acc: 0.8924\n",
            "66292/66292 [==============================] - 3s 48us/step\n",
            "\n",
            "FOLD 4\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124609 samples, validate on 31451 samples\n",
            "Epoch 1/15\n",
            "124609/124609 [==============================] - 61s 489us/step - loss: 0.2718 - acc: 0.8770 - val_loss: 0.2355 - val_acc: 0.8946\n",
            "Epoch 2/15\n",
            "124609/124609 [==============================] - 61s 490us/step - loss: 0.2693 - acc: 0.8779 - val_loss: 0.2391 - val_acc: 0.8937\n",
            "Epoch 3/15\n",
            "124609/124609 [==============================] - 61s 491us/step - loss: 0.2682 - acc: 0.8788 - val_loss: 0.2409 - val_acc: 0.8921\n",
            "66292/66292 [==============================] - 3s 48us/step\n",
            "\n",
            "FOLD 5\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124771 samples, validate on 31289 samples\n",
            "Epoch 1/15\n",
            "124771/124771 [==============================] - 61s 489us/step - loss: 0.2695 - acc: 0.8780 - val_loss: 0.2309 - val_acc: 0.8969\n",
            "Epoch 2/15\n",
            "124771/124771 [==============================] - 61s 492us/step - loss: 0.2673 - acc: 0.8791 - val_loss: 0.2326 - val_acc: 0.8955\n",
            "Epoch 3/15\n",
            "124771/124771 [==============================] - 61s 491us/step - loss: 0.2664 - acc: 0.8798 - val_loss: 0.2360 - val_acc: 0.8939\n",
            "66292/66292 [==============================] - 3s 49us/step\n",
            "\n",
            "Save model after cross-validation...\n",
            "Make the submission ready...\n",
            "66292/66292 [==============================] - 3s 48us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w_Qgo7qZjPbh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Model 2: Embedding + LSTM + multi CNN\n"
      ]
    },
    {
      "metadata": {
        "id": "Xby86sUKw4yg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    input_words = Input((max_len, ))\n",
        "    x_words = Embedding(19479, embed_size,weights=[embedding_matrix],trainable=False)(input_words)\n",
        "    x_words = SpatialDropout1D(0.3)(x_words)\n",
        "    x_words = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x_words)\n",
        "    x_words = Dropout(0.2)(x_words)\n",
        "    x_words = Conv1D(256, 3, strides = 1,  padding='causal', activation='relu', )(x_words)\n",
        "    x_words = Conv1D(128, 3, strides = 1,  padding='causal', activation='relu', )(x_words)\n",
        "    x_words = Conv1D(64, 3, strides = 1,   padding='causal', activation='relu', )(x_words)\n",
        "    x_words = GlobalMaxPool1D()(x_words)\n",
        "    x_words = Dropout(0.2)(x_words)\n",
        "\n",
        "    x = Dense(50, activation=\"relu\")(x_words)\n",
        "    x = Dropout(0.2)(x)\n",
        "    predictions = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=[input_words], outputs=predictions)\n",
        "    model.compile(optimizer='nadam' ,loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZB9ZOYE47gN",
        "colab_type": "code",
        "outputId": "5120f18e-5dfc-4465-8007-06d6b18089e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1275
        }
      },
      "cell_type": "code",
      "source": [
        "test_preds = np.zeros((test.shape[0], 5))\n",
        "file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                              save_best_only = True, mode = \"min\")\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode=\"min\", patience = 3, verbose=1)\n",
        "\n",
        "print(\"Building the model...\")\n",
        "model2 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    print(\"FOLD\", i+1)    \n",
        "    print(\"Splitting the data into train and validation...\")\n",
        "    train_seq, val_seq = X_train[train[\"fold_id\"] != i], X_train[train[\"fold_id\"] == i]\n",
        "    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
        "    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))        \n",
        "    \n",
        "    print(\"Training the model...\")\n",
        "    model2.fit(train_seq, y_train, validation_data = (val_seq, y_val), batch_size = 128, epochs = 15, verbose = 1, callbacks = [early_stop]) \n",
        "    model2.save_weights(file_path)  \n",
        "    test_preds += model3.predict([X_test], batch_size=1024, verbose=1)    \n",
        "    print()\n",
        "    \n",
        "print(\"Save model after cross-validation...\")\n",
        "model2.save_weights(file_path)   \n",
        "test_preds /= NUM_FOLDS\n",
        "\n",
        "\n",
        "print(\"Make the submission ready...\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "\n",
        "pred = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/drive/My Drive/DeepLearning/blend.csv\", index=False)\n",
        "\n",
        "predictions = np.round(np.argmax(test_preds, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/drive/My Drive/DeepLearning/avg_blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the model...\n",
            "FOLD 1\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 125230 samples, validate on 30830 samples\n",
            "Epoch 1/15\n",
            "125230/125230 [==============================] - 46s 366us/step - loss: 0.9709 - acc: 0.5990 - val_loss: 0.8935 - val_acc: 0.6251\n",
            "Epoch 2/15\n",
            "125230/125230 [==============================] - 40s 322us/step - loss: 0.8800 - acc: 0.6357 - val_loss: 0.8489 - val_acc: 0.6488\n",
            "Epoch 3/15\n",
            "125230/125230 [==============================] - 40s 321us/step - loss: 0.8414 - acc: 0.6511 - val_loss: 0.8371 - val_acc: 0.6549\n",
            "Epoch 4/15\n",
            "125230/125230 [==============================] - 40s 321us/step - loss: 0.8159 - acc: 0.6616 - val_loss: 0.8279 - val_acc: 0.6561\n",
            "Epoch 5/15\n",
            "125230/125230 [==============================] - 40s 321us/step - loss: 0.7955 - acc: 0.6676 - val_loss: 0.8275 - val_acc: 0.6587\n",
            "Epoch 6/15\n",
            "125230/125230 [==============================] - 40s 322us/step - loss: 0.7816 - acc: 0.6741 - val_loss: 0.8235 - val_acc: 0.6600\n",
            "Epoch 7/15\n",
            "125230/125230 [==============================] - 40s 323us/step - loss: 0.7671 - acc: 0.6804 - val_loss: 0.8327 - val_acc: 0.6608\n",
            "Epoch 8/15\n",
            "125230/125230 [==============================] - 40s 322us/step - loss: 0.7576 - acc: 0.6847 - val_loss: 0.8334 - val_acc: 0.6623\n",
            "66292/66292 [==============================] - 4s 54us/step\n",
            "\n",
            "FOLD 2\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124608 samples, validate on 31452 samples\n",
            "Epoch 1/15\n",
            "124608/124608 [==============================] - 41s 326us/step - loss: 0.7761 - acc: 0.6783 - val_loss: 0.7067 - val_acc: 0.7130\n",
            "Epoch 2/15\n",
            "124608/124608 [==============================] - 40s 322us/step - loss: 0.7584 - acc: 0.6843 - val_loss: 0.7185 - val_acc: 0.7009\n",
            "Epoch 3/15\n",
            "124608/124608 [==============================] - 40s 322us/step - loss: 0.7486 - acc: 0.6889 - val_loss: 0.7210 - val_acc: 0.7015\n",
            "66292/66292 [==============================] - 3s 47us/step\n",
            "\n",
            "FOLD 3\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 125022 samples, validate on 31038 samples\n",
            "Epoch 1/15\n",
            "125022/125022 [==============================] - 41s 325us/step - loss: 0.7590 - acc: 0.6859 - val_loss: 0.6695 - val_acc: 0.7249\n",
            "Epoch 2/15\n",
            "125022/125022 [==============================] - 40s 321us/step - loss: 0.7492 - acc: 0.6886 - val_loss: 0.6859 - val_acc: 0.7142\n",
            "Epoch 3/15\n",
            "125022/125022 [==============================] - 40s 320us/step - loss: 0.7390 - acc: 0.6918 - val_loss: 0.6826 - val_acc: 0.7144\n",
            "66292/66292 [==============================] - 3s 48us/step\n",
            "\n",
            "FOLD 4\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124609 samples, validate on 31451 samples\n",
            "Epoch 1/15\n",
            "124609/124609 [==============================] - 41s 327us/step - loss: 0.7406 - acc: 0.6922 - val_loss: 0.6621 - val_acc: 0.7266\n",
            "Epoch 2/15\n",
            "124609/124609 [==============================] - 40s 322us/step - loss: 0.7346 - acc: 0.6959 - val_loss: 0.6731 - val_acc: 0.7186\n",
            "Epoch 3/15\n",
            "124609/124609 [==============================] - 40s 321us/step - loss: 0.7292 - acc: 0.6982 - val_loss: 0.6777 - val_acc: 0.7146\n",
            "66292/66292 [==============================] - 3s 47us/step\n",
            "\n",
            "FOLD 5\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 124771 samples, validate on 31289 samples\n",
            "Epoch 1/15\n",
            "124771/124771 [==============================] - 41s 326us/step - loss: 0.7309 - acc: 0.6980 - val_loss: 0.6523 - val_acc: 0.7281\n",
            "Epoch 2/15\n",
            "124771/124771 [==============================] - 40s 321us/step - loss: 0.7263 - acc: 0.6981 - val_loss: 0.6621 - val_acc: 0.7269\n",
            "Epoch 3/15\n",
            "124771/124771 [==============================] - 40s 321us/step - loss: 0.7205 - acc: 0.6998 - val_loss: 0.6625 - val_acc: 0.7189\n",
            "66292/66292 [==============================] - 3s 48us/step\n",
            "\n",
            "Save model after cross-validation...\n",
            "Make the submission ready...\n",
            "66292/66292 [==============================] - 3s 47us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9I07QVa8OYB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Model 3: Embedding + {LSTM/GRU} + multi CNN"
      ]
    },
    {
      "metadata": {
        "id": "rpqwTRtOn3Be",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(128, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_gru)\n",
        "    x_conv1 = Conv1D(64, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_conv1)\n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_conv1)\n",
        "\n",
        "      \n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    \n",
        "    x_conv2 = Conv1D(128, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_gru)\n",
        "    x_conv2 = Conv1D(64, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_conv2)\n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_conv2)    \n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    \n",
        "    \n",
        "    x_conv3 = Conv1D(128, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_lstm)\n",
        "    x_conv3 = Conv1D(64, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_conv3)\n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='causal', kernel_initializer='he_uniform')(x_conv3)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    x_conv4 = Conv1D(128, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_lstm)\n",
        "    x_conv4 = Conv1D(64, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_conv4)\n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='causal', kernel_initializer='he_uniform')(x_conv4)\n",
        "\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZamFj3U7z5o",
        "colab_type": "code",
        "outputId": "2b0ec71d-a9d5-4d66-c75a-768d9ddcdcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3134
        }
      },
      "cell_type": "code",
      "source": [
        "trained_model3 = build_model3(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)\n",
        "trained_model3.summary()\n",
        "SVG(model_to_dot(trained_model3,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 50, 300)      5843700     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2 (SpatialDro (None, 50, 300)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 50, 128)      140544      spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 50, 128)      187392      spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 50, 128)      49280       bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 50, 128)      49280       bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 50, 128)      49280       bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 50, 128)      49280       bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 50, 64)       24640       conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 50, 64)       24640       conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 50, 64)       24640       conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 50, 64)       24640       conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 50, 32)       6176        conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 50, 32)       6176        conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 50, 32)       6176        conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 50, 32)       6176        conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_5 (Glo (None, 32)           0           conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 32)           0           conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_6 (Glo (None, 32)           0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 32)           0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_7 (Glo (None, 32)           0           conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 32)           0           conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_8 (Glo (None, 32)           0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 32)           0           conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_5[0][0] \n",
            "                                                                 global_max_pooling1d_5[0][0]     \n",
            "                                                                 global_average_pooling1d_6[0][0] \n",
            "                                                                 global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_average_pooling1d_7[0][0] \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_average_pooling1d_8[0][0] \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 256)          1024        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           16448       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64)           256         dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 32)           2080        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 5)            165         dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,511,993\n",
            "Trainable params: 667,653\n",
            "Non-trainable params: 5,844,340\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"1300pt\" viewBox=\"0.00 0.00 4006.00 1300.00\" width=\"4006pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1296)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1296 4002,-1296 4002,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140252887122888 -->\n<g class=\"node\" id=\"node1\">\n<title>140252887122888</title>\n<polygon fill=\"none\" points=\"2039.5,-1245.5 2039.5,-1291.5 2310.5,-1291.5 2310.5,-1245.5 2039.5,-1245.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2106\" y=\"-1264.8\">input_2: InputLayer</text>\n<polyline fill=\"none\" points=\"2172.5,-1245.5 2172.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2201.5\" y=\"-1276.3\">input:</text>\n<polyline fill=\"none\" points=\"2172.5,-1268.5 2230.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2201.5\" y=\"-1253.3\">output:</text>\n<polyline fill=\"none\" points=\"2230.5,-1245.5 2230.5,-1291.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2270.5\" y=\"-1276.3\">(None, 50)</text>\n<polyline fill=\"none\" points=\"2230.5,-1268.5 2310.5,-1268.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2270.5\" y=\"-1253.3\">(None, 50)</text>\n</g>\n<!-- 140252887303392 -->\n<g class=\"node\" id=\"node2\">\n<title>140252887303392</title>\n<polygon fill=\"none\" points=\"2005.5,-1162.5 2005.5,-1208.5 2344.5,-1208.5 2344.5,-1162.5 2005.5,-1162.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091\" y=\"-1181.8\">embedding_2: Embedding</text>\n<polyline fill=\"none\" points=\"2176.5,-1162.5 2176.5,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2205.5\" y=\"-1193.3\">input:</text>\n<polyline fill=\"none\" points=\"2176.5,-1185.5 2234.5,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2205.5\" y=\"-1170.3\">output:</text>\n<polyline fill=\"none\" points=\"2234.5,-1162.5 2234.5,-1208.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2289.5\" y=\"-1193.3\">(None, 50)</text>\n<polyline fill=\"none\" points=\"2234.5,-1185.5 2344.5,-1185.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2289.5\" y=\"-1170.3\">(None, 50, 300)</text>\n</g>\n<!-- 140252887122888&#45;&gt;140252887303392 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140252887122888-&gt;140252887303392</title>\n<path d=\"M2175,-1245.3799C2175,-1237.1745 2175,-1227.7679 2175,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2178.5001,-1218.784 2175,-1208.784 2171.5001,-1218.784 2178.5001,-1218.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252887303728 -->\n<g class=\"node\" id=\"node3\">\n<title>140252887303728</title>\n<polygon fill=\"none\" points=\"1966.5,-1079.5 1966.5,-1125.5 2383.5,-1125.5 2383.5,-1079.5 1966.5,-1079.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091\" y=\"-1098.8\">spatial_dropout1d_2: SpatialDropout1D</text>\n<polyline fill=\"none\" points=\"2215.5,-1079.5 2215.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2244.5\" y=\"-1110.3\">input:</text>\n<polyline fill=\"none\" points=\"2215.5,-1102.5 2273.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2244.5\" y=\"-1087.3\">output:</text>\n<polyline fill=\"none\" points=\"2273.5,-1079.5 2273.5,-1125.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2328.5\" y=\"-1110.3\">(None, 50, 300)</text>\n<polyline fill=\"none\" points=\"2273.5,-1102.5 2383.5,-1102.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2328.5\" y=\"-1087.3\">(None, 50, 300)</text>\n</g>\n<!-- 140252887303392&#45;&gt;140252887303728 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140252887303392-&gt;140252887303728</title>\n<path d=\"M2175,-1162.3799C2175,-1154.1745 2175,-1144.7679 2175,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2178.5001,-1135.784 2175,-1125.784 2171.5001,-1135.784 2178.5001,-1135.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252886456976 -->\n<g class=\"node\" id=\"node4\">\n<title>140252886456976</title>\n<polygon fill=\"none\" points=\"1409.5,-996.5 1409.5,-1042.5 1936.5,-1042.5 1936.5,-996.5 1409.5,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1589\" y=\"-1015.8\">bidirectional_3(cu_dnngru_2): Bidirectional(CuDNNGRU)</text>\n<polyline fill=\"none\" points=\"1768.5,-996.5 1768.5,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1797.5\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"1768.5,-1019.5 1826.5,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1797.5\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"1826.5,-996.5 1826.5,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1881.5\" y=\"-1027.3\">(None, 50, 300)</text>\n<polyline fill=\"none\" points=\"1826.5,-1019.5 1936.5,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1881.5\" y=\"-1004.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252887303728&#45;&gt;140252886456976 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140252887303728-&gt;140252886456976</title>\n<path d=\"M2035.8317,-1079.4901C1969.4565,-1068.5157 1889.9664,-1055.3729 1822.3293,-1044.1899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1822.8761,-1040.7329 1812.4391,-1042.5547 1821.7342,-1047.6391 1822.8761,-1040.7329\" stroke=\"#000000\"/>\n</g>\n<!-- 140252876423856 -->\n<g class=\"node\" id=\"node5\">\n<title>140252876423856</title>\n<polygon fill=\"none\" points=\"2266,-996.5 2266,-1042.5 2806,-1042.5 2806,-996.5 2266,-996.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2452\" y=\"-1015.8\">bidirectional_4(cu_dnnlstm_2): Bidirectional(CuDNNLSTM)</text>\n<polyline fill=\"none\" points=\"2638,-996.5 2638,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2667\" y=\"-1027.3\">input:</text>\n<polyline fill=\"none\" points=\"2638,-1019.5 2696,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2667\" y=\"-1004.3\">output:</text>\n<polyline fill=\"none\" points=\"2696,-996.5 2696,-1042.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2751\" y=\"-1027.3\">(None, 50, 300)</text>\n<polyline fill=\"none\" points=\"2696,-1019.5 2806,-1019.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2751\" y=\"-1004.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252887303728&#45;&gt;140252876423856 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140252887303728-&gt;140252876423856</title>\n<path d=\"M2275.0792,-1079.4901C2321.9309,-1068.7181 2377.8691,-1055.857 2425.9141,-1044.8106\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2426.7646,-1048.2065 2435.7261,-1042.5547 2425.1961,-1041.3844 2426.7646,-1048.2065\" stroke=\"#000000\"/>\n</g>\n<!-- 140252876426264 -->\n<g class=\"node\" id=\"node6\">\n<title>140252876426264</title>\n<polygon fill=\"none\" points=\"1024,-913.5 1024,-959.5 1332,-959.5 1332,-913.5 1024,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-932.8\">conv1d_13: Conv1D</text>\n<polyline fill=\"none\" points=\"1164,-913.5 1164,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1193\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"1164,-936.5 1222,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1193\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"1222,-913.5 1222,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1277\" y=\"-944.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"1222,-936.5 1332,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1277\" y=\"-921.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252886456976&#45;&gt;140252876426264 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140252886456976-&gt;140252876426264</title>\n<path d=\"M1535.7723,-996.4901C1470.4568,-985.5382 1392.2621,-972.4268 1325.6572,-961.2587\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1325.9358,-957.7566 1315.4947,-959.5547 1324.7782,-964.6602 1325.9358,-957.7566\" stroke=\"#000000\"/>\n</g>\n<!-- 140252874037792 -->\n<g class=\"node\" id=\"node7\">\n<title>140252874037792</title>\n<polygon fill=\"none\" points=\"1519,-913.5 1519,-959.5 1827,-959.5 1827,-913.5 1519,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1589\" y=\"-932.8\">conv1d_16: Conv1D</text>\n<polyline fill=\"none\" points=\"1659,-913.5 1659,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1688\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"1659,-936.5 1717,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1688\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"1717,-913.5 1717,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-944.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"1717,-936.5 1827,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-921.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252886456976&#45;&gt;140252874037792 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140252886456976-&gt;140252874037792</title>\n<path d=\"M1673,-996.3799C1673,-988.1745 1673,-978.7679 1673,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1676.5001,-969.784 1673,-959.784 1669.5001,-969.784 1676.5001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873065528 -->\n<g class=\"node\" id=\"node8\">\n<title>140252873065528</title>\n<polygon fill=\"none\" points=\"2382,-913.5 2382,-959.5 2690,-959.5 2690,-913.5 2382,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2452\" y=\"-932.8\">conv1d_19: Conv1D</text>\n<polyline fill=\"none\" points=\"2522,-913.5 2522,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2551\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"2522,-936.5 2580,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2551\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"2580,-913.5 2580,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-944.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"2580,-936.5 2690,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-921.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252876423856&#45;&gt;140252873065528 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140252876423856-&gt;140252873065528</title>\n<path d=\"M2536,-996.3799C2536,-988.1745 2536,-978.7679 2536,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2539.5001,-969.784 2536,-959.784 2532.5001,-969.784 2539.5001,-969.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872716360 -->\n<g class=\"node\" id=\"node9\">\n<title>140252872716360</title>\n<polygon fill=\"none\" points=\"2909,-913.5 2909,-959.5 3217,-959.5 3217,-913.5 2909,-913.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2979\" y=\"-932.8\">conv1d_22: Conv1D</text>\n<polyline fill=\"none\" points=\"3049,-913.5 3049,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3078\" y=\"-944.3\">input:</text>\n<polyline fill=\"none\" points=\"3049,-936.5 3107,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3078\" y=\"-921.3\">output:</text>\n<polyline fill=\"none\" points=\"3107,-913.5 3107,-959.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3162\" y=\"-944.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"3107,-936.5 3217,-936.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3162\" y=\"-921.3\">(None, 50, 128)</text>\n</g>\n<!-- 140252876423856&#45;&gt;140252872716360 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140252876423856-&gt;140252872716360</title>\n<path d=\"M2682.099,-996.4901C2751.9225,-985.4932 2835.5706,-972.319 2906.6703,-961.1212\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2907.2831,-964.5679 2916.6168,-959.5547 2906.194,-957.6531 2907.2831,-964.5679\" stroke=\"#000000\"/>\n</g>\n<!-- 140252876426488 -->\n<g class=\"node\" id=\"node10\">\n<title>140252876426488</title>\n<polygon fill=\"none\" points=\"685,-830.5 685,-876.5 993,-876.5 993,-830.5 685,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"755\" y=\"-849.8\">conv1d_14: Conv1D</text>\n<polyline fill=\"none\" points=\"825,-830.5 825,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"854\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"825,-853.5 883,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"854\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"883,-830.5 883,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"938\" y=\"-861.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"883,-853.5 993,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"938\" y=\"-838.3\">(None, 50, 64)</text>\n</g>\n<!-- 140252876426264&#45;&gt;140252876426488 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140252876426264-&gt;140252876426488</title>\n<path d=\"M1084.0198,-913.4901C1040.2071,-902.7631 987.9326,-889.9643 942.9426,-878.9491\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"943.7085,-875.5333 933.163,-876.5547 942.0438,-882.3325 943.7085,-875.5333\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873658440 -->\n<g class=\"node\" id=\"node11\">\n<title>140252873658440</title>\n<polygon fill=\"none\" points=\"1519,-830.5 1519,-876.5 1827,-876.5 1827,-830.5 1519,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1589\" y=\"-849.8\">conv1d_17: Conv1D</text>\n<polyline fill=\"none\" points=\"1659,-830.5 1659,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1688\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"1659,-853.5 1717,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1688\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"1717,-830.5 1717,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-861.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"1717,-853.5 1827,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-838.3\">(None, 50, 64)</text>\n</g>\n<!-- 140252874037792&#45;&gt;140252873658440 -->\n<g class=\"edge\" id=\"edge10\">\n<title>140252874037792-&gt;140252873658440</title>\n<path d=\"M1673,-913.3799C1673,-905.1745 1673,-895.7679 1673,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1676.5001,-886.784 1673,-876.784 1669.5001,-886.784 1676.5001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873301800 -->\n<g class=\"node\" id=\"node12\">\n<title>140252873301800</title>\n<polygon fill=\"none\" points=\"2382,-830.5 2382,-876.5 2690,-876.5 2690,-830.5 2382,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2452\" y=\"-849.8\">conv1d_20: Conv1D</text>\n<polyline fill=\"none\" points=\"2522,-830.5 2522,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2551\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"2522,-853.5 2580,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2551\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"2580,-830.5 2580,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-861.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"2580,-853.5 2690,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-838.3\">(None, 50, 64)</text>\n</g>\n<!-- 140252873065528&#45;&gt;140252873301800 -->\n<g class=\"edge\" id=\"edge11\">\n<title>140252873065528-&gt;140252873301800</title>\n<path d=\"M2536,-913.3799C2536,-905.1745 2536,-895.7679 2536,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2539.5001,-886.784 2536,-876.784 2532.5001,-886.784 2539.5001,-886.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872318312 -->\n<g class=\"node\" id=\"node13\">\n<title>140252872318312</title>\n<polygon fill=\"none\" points=\"3010,-830.5 3010,-876.5 3318,-876.5 3318,-830.5 3010,-830.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3080\" y=\"-849.8\">conv1d_23: Conv1D</text>\n<polyline fill=\"none\" points=\"3150,-830.5 3150,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3179\" y=\"-861.3\">input:</text>\n<polyline fill=\"none\" points=\"3150,-853.5 3208,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3179\" y=\"-838.3\">output:</text>\n<polyline fill=\"none\" points=\"3208,-830.5 3208,-876.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3263\" y=\"-861.3\">(None, 50, 128)</text>\n<polyline fill=\"none\" points=\"3208,-853.5 3318,-853.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3263\" y=\"-838.3\">(None, 50, 64)</text>\n</g>\n<!-- 140252872716360&#45;&gt;140252872318312 -->\n<g class=\"edge\" id=\"edge12\">\n<title>140252872716360-&gt;140252872318312</title>\n<path d=\"M3091.1341,-913.3799C3102.5298,-904.0151 3115.8297,-893.0855 3127.9245,-883.1462\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"3130.1627,-885.8371 3135.6665,-876.784 3125.7183,-880.429 3130.1627,-885.8371\" stroke=\"#000000\"/>\n</g>\n<!-- 140252874209656 -->\n<g class=\"node\" id=\"node14\">\n<title>140252874209656</title>\n<polygon fill=\"none\" points=\"604,-747.5 604,-793.5 904,-793.5 904,-747.5 604,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674\" y=\"-766.8\">conv1d_15: Conv1D</text>\n<polyline fill=\"none\" points=\"744,-747.5 744,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"773\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"744,-770.5 802,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"773\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"802,-747.5 802,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"853\" y=\"-778.3\">(None, 50, 64)</text>\n<polyline fill=\"none\" points=\"802,-770.5 904,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"853\" y=\"-755.3\">(None, 50, 32)</text>\n</g>\n<!-- 140252876426488&#45;&gt;140252874209656 -->\n<g class=\"edge\" id=\"edge13\">\n<title>140252876426488-&gt;140252874209656</title>\n<path d=\"M815.3228,-830.3799C806.0063,-821.2827 795.1777,-810.7088 785.2354,-801.0005\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"787.4451,-798.2663 777.8451,-793.784 782.5546,-803.2746 787.4451,-798.2663\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873759488 -->\n<g class=\"node\" id=\"node15\">\n<title>140252873759488</title>\n<polygon fill=\"none\" points=\"1523,-747.5 1523,-793.5 1823,-793.5 1823,-747.5 1523,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1593\" y=\"-766.8\">conv1d_18: Conv1D</text>\n<polyline fill=\"none\" points=\"1663,-747.5 1663,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1692\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"1663,-770.5 1721,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1692\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"1721,-747.5 1721,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-778.3\">(None, 50, 64)</text>\n<polyline fill=\"none\" points=\"1721,-770.5 1823,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1772\" y=\"-755.3\">(None, 50, 32)</text>\n</g>\n<!-- 140252873658440&#45;&gt;140252873759488 -->\n<g class=\"edge\" id=\"edge14\">\n<title>140252873658440-&gt;140252873759488</title>\n<path d=\"M1673,-830.3799C1673,-822.1745 1673,-812.7679 1673,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1676.5001,-803.784 1673,-793.784 1669.5001,-803.784 1676.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872917400 -->\n<g class=\"node\" id=\"node16\">\n<title>140252872917400</title>\n<polygon fill=\"none\" points=\"2386,-747.5 2386,-793.5 2686,-793.5 2686,-747.5 2386,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2456\" y=\"-766.8\">conv1d_21: Conv1D</text>\n<polyline fill=\"none\" points=\"2526,-747.5 2526,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2555\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"2526,-770.5 2584,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2555\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"2584,-747.5 2584,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-778.3\">(None, 50, 64)</text>\n<polyline fill=\"none\" points=\"2584,-770.5 2686,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2635\" y=\"-755.3\">(None, 50, 32)</text>\n</g>\n<!-- 140252873301800&#45;&gt;140252872917400 -->\n<g class=\"edge\" id=\"edge15\">\n<title>140252873301800-&gt;140252872917400</title>\n<path d=\"M2536,-830.3799C2536,-822.1745 2536,-812.7679 2536,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2539.5001,-803.784 2536,-793.784 2532.5001,-803.784 2539.5001,-803.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872427000 -->\n<g class=\"node\" id=\"node17\">\n<title>140252872427000</title>\n<polygon fill=\"none\" points=\"3114,-747.5 3114,-793.5 3414,-793.5 3414,-747.5 3114,-747.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3184\" y=\"-766.8\">conv1d_24: Conv1D</text>\n<polyline fill=\"none\" points=\"3254,-747.5 3254,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3283\" y=\"-778.3\">input:</text>\n<polyline fill=\"none\" points=\"3254,-770.5 3312,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3283\" y=\"-755.3\">output:</text>\n<polyline fill=\"none\" points=\"3312,-747.5 3312,-793.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3363\" y=\"-778.3\">(None, 50, 64)</text>\n<polyline fill=\"none\" points=\"3312,-770.5 3414,-770.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3363\" y=\"-755.3\">(None, 50, 32)</text>\n</g>\n<!-- 140252872318312&#45;&gt;140252872427000 -->\n<g class=\"edge\" id=\"edge16\">\n<title>140252872318312-&gt;140252872427000</title>\n<path d=\"M3191.8556,-830.3799C3203.031,-821.1043 3216.056,-810.2936 3227.9392,-800.4304\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"3230.4875,-802.8639 3235.947,-793.784 3226.0168,-797.4775 3230.4875,-802.8639\" stroke=\"#000000\"/>\n</g>\n<!-- 140252874312728 -->\n<g class=\"node\" id=\"node18\">\n<title>140252874312728</title>\n<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 504,-710.5 504,-664.5 0,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-683.8\">global_average_pooling1d_5: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"344,-664.5 344,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"344,-687.5 402,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"402,-664.5 402,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"402,-687.5 504,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252874209656&#45;&gt;140252874312728 -->\n<g class=\"edge\" id=\"edge17\">\n<title>140252874209656-&gt;140252874312728</title>\n<path d=\"M614.8317,-747.4901C548.4565,-736.5157 468.9664,-723.3729 401.3293,-712.1899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"401.8761,-708.7329 391.4391,-710.5547 400.7342,-715.6391 401.8761,-708.7329\" stroke=\"#000000\"/>\n</g>\n<!-- 140252874035328 -->\n<g class=\"node\" id=\"node19\">\n<title>140252874035328</title>\n<polygon fill=\"none\" points=\"522,-664.5 522,-710.5 986,-710.5 986,-664.5 522,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674\" y=\"-683.8\">global_max_pooling1d_5: GlobalMaxPooling1D</text>\n<polyline fill=\"none\" points=\"826,-664.5 826,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"826,-687.5 884,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"884,-664.5 884,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"884,-687.5 986,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252874209656&#45;&gt;140252874035328 -->\n<g class=\"edge\" id=\"edge18\">\n<title>140252874209656-&gt;140252874035328</title>\n<path d=\"M754,-747.3799C754,-739.1745 754,-729.7679 754,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"757.5001,-720.784 754,-710.784 750.5001,-720.784 757.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873362624 -->\n<g class=\"node\" id=\"node20\">\n<title>140252873362624</title>\n<polygon fill=\"none\" points=\"1004,-664.5 1004,-710.5 1508,-710.5 1508,-664.5 1004,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1176\" y=\"-683.8\">global_average_pooling1d_6: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"1348,-664.5 1348,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1377\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"1348,-687.5 1406,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1377\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"1406,-664.5 1406,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1457\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"1406,-687.5 1508,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1457\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252873759488&#45;&gt;140252873362624 -->\n<g class=\"edge\" id=\"edge19\">\n<title>140252873759488-&gt;140252873362624</title>\n<path d=\"M1557.396,-747.4901C1502.8246,-736.6282 1437.581,-723.642 1381.7745,-712.5343\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1382.3197,-709.0742 1371.8289,-710.5547 1380.9532,-715.9395 1382.3197,-709.0742\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873065864 -->\n<g class=\"node\" id=\"node21\">\n<title>140252873065864</title>\n<polygon fill=\"none\" points=\"1526,-664.5 1526,-710.5 1990,-710.5 1990,-664.5 1526,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1678\" y=\"-683.8\">global_max_pooling1d_6: GlobalMaxPooling1D</text>\n<polyline fill=\"none\" points=\"1830,-664.5 1830,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1859\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"1830,-687.5 1888,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1859\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"1888,-664.5 1888,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1939\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"1888,-687.5 1990,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1939\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252873759488&#45;&gt;140252873065864 -->\n<g class=\"edge\" id=\"edge20\">\n<title>140252873759488-&gt;140252873065864</title>\n<path d=\"M1696.6772,-747.3799C1705.9937,-738.2827 1716.8223,-727.7088 1726.7646,-718.0005\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1729.4454,-720.2746 1734.1549,-710.784 1724.5549,-715.2663 1729.4454,-720.2746\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872996792 -->\n<g class=\"node\" id=\"node22\">\n<title>140252872996792</title>\n<polygon fill=\"none\" points=\"2008,-664.5 2008,-710.5 2512,-710.5 2512,-664.5 2008,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2180\" y=\"-683.8\">global_average_pooling1d_7: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"2352,-664.5 2352,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2381\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"2352,-687.5 2410,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2381\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"2410,-664.5 2410,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2461\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"2410,-687.5 2512,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2461\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252872917400&#45;&gt;140252872996792 -->\n<g class=\"edge\" id=\"edge21\">\n<title>140252872917400-&gt;140252872996792</title>\n<path d=\"M2459.4851,-747.4901C2424.3381,-736.9205 2382.5025,-724.3395 2346.2422,-713.4352\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2347.248,-710.0829 2336.6637,-710.5547 2345.2321,-716.7863 2347.248,-710.0829\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872718656 -->\n<g class=\"node\" id=\"node23\">\n<title>140252872718656</title>\n<polygon fill=\"none\" points=\"2530,-664.5 2530,-710.5 2994,-710.5 2994,-664.5 2530,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2682\" y=\"-683.8\">global_max_pooling1d_7: GlobalMaxPooling1D</text>\n<polyline fill=\"none\" points=\"2834,-664.5 2834,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2863\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"2834,-687.5 2892,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2863\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"2892,-664.5 2892,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2943\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"2892,-687.5 2994,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2943\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252872917400&#45;&gt;140252872718656 -->\n<g class=\"edge\" id=\"edge22\">\n<title>140252872917400-&gt;140252872718656</title>\n<path d=\"M2598.6535,-747.4901C2626.821,-737.1454 2660.2347,-724.874 2689.4801,-714.1334\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2691.0443,-717.2876 2699.2246,-710.5547 2688.631,-710.7167 2691.0443,-717.2876\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872027384 -->\n<g class=\"node\" id=\"node24\">\n<title>140252872027384</title>\n<polygon fill=\"none\" points=\"3012,-664.5 3012,-710.5 3516,-710.5 3516,-664.5 3012,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3184\" y=\"-683.8\">global_average_pooling1d_8: GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"3356,-664.5 3356,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3385\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"3356,-687.5 3414,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3385\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"3414,-664.5 3414,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3465\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"3414,-687.5 3516,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3465\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252872427000&#45;&gt;140252872027384 -->\n<g class=\"edge\" id=\"edge23\">\n<title>140252872427000-&gt;140252872027384</title>\n<path d=\"M3264,-747.3799C3264,-739.1745 3264,-729.7679 3264,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"3267.5001,-720.784 3264,-710.784 3260.5001,-720.784 3267.5001,-720.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872257888 -->\n<g class=\"node\" id=\"node25\">\n<title>140252872257888</title>\n<polygon fill=\"none\" points=\"3534,-664.5 3534,-710.5 3998,-710.5 3998,-664.5 3534,-664.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3686\" y=\"-683.8\">global_max_pooling1d_8: GlobalMaxPooling1D</text>\n<polyline fill=\"none\" points=\"3838,-664.5 3838,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3867\" y=\"-695.3\">input:</text>\n<polyline fill=\"none\" points=\"3838,-687.5 3896,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3867\" y=\"-672.3\">output:</text>\n<polyline fill=\"none\" points=\"3896,-664.5 3896,-710.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3947\" y=\"-695.3\">(None, 50, 32)</text>\n<polyline fill=\"none\" points=\"3896,-687.5 3998,-687.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"3947\" y=\"-672.3\">(None, 32)</text>\n</g>\n<!-- 140252872427000&#45;&gt;140252872257888 -->\n<g class=\"edge\" id=\"edge24\">\n<title>140252872427000-&gt;140252872257888</title>\n<path d=\"M3403.1683,-747.4901C3469.5435,-736.5157 3549.0336,-723.3729 3616.6707,-712.1899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"3617.2658,-715.6391 3626.5609,-710.5547 3616.1239,-708.7329 3617.2658,-715.6391\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872260352 -->\n<g class=\"node\" id=\"node26\">\n<title>140252872260352</title>\n<polygon fill=\"none\" points=\"1601.5,-581.5 1601.5,-627.5 2416.5,-627.5 2416.5,-581.5 1601.5,-581.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1689\" y=\"-600.8\">concatenate_2: Concatenate</text>\n<polyline fill=\"none\" points=\"1776.5,-581.5 1776.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1805.5\" y=\"-612.3\">input:</text>\n<polyline fill=\"none\" points=\"1776.5,-604.5 1834.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1805.5\" y=\"-589.3\">output:</text>\n<polyline fill=\"none\" points=\"1834.5,-581.5 1834.5,-627.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2125.5\" y=\"-612.3\">[(None, 32), (None, 32), (None, 32), (None, 32), (None, 32), (None, 32), (None, 32), (None, 32)]</text>\n<polyline fill=\"none\" points=\"1834.5,-604.5 2416.5,-604.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2125.5\" y=\"-589.3\">(None, 256)</text>\n</g>\n<!-- 140252874312728&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge25\">\n<title>140252874312728-&gt;140252872260352</title>\n<path d=\"M504.0945,-664.5901C507.0795,-664.3883 510.049,-664.1914 513,-664 876.7058,-640.4083 1290.8022,-624.8098 1590.9716,-615.526\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1591.2679,-619.0187 1601.1553,-615.2121 1591.0522,-612.022 1591.2679,-619.0187\" stroke=\"#000000\"/>\n</g>\n<!-- 140252874035328&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge26\">\n<title>140252874035328-&gt;140252872260352</title>\n<path d=\"M986.1427,-664.7018C989.1127,-664.4631 992.0662,-664.229 995,-664 1190.8309,-648.7124 1408.217,-635.4638 1591.3256,-625.3741\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1591.6046,-628.8641 1601.3974,-624.8204 1591.2203,-621.8747 1591.6046,-628.8641\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873362624&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge27\">\n<title>140252873362624-&gt;140252872260352</title>\n<path d=\"M1464.7525,-664.4901C1565.9507,-653.3354 1687.473,-639.9406 1789.9912,-628.6404\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1790.6116,-632.0933 1800.168,-627.5187 1789.8447,-625.1355 1790.6116,-632.0933\" stroke=\"#000000\"/>\n</g>\n<!-- 140252873065864&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge28\">\n<title>140252873065864-&gt;140252872260352</title>\n<path d=\"M1827.5842,-664.4901C1859.2756,-654.0105 1896.9466,-641.5535 1929.7265,-630.7139\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"1930.885,-634.0173 1939.2805,-627.5547 1928.6872,-627.3713 1930.885,-634.0173\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872996792&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge29\">\n<title>140252872996792-&gt;140252872260352</title>\n<path d=\"M2190.4158,-664.4901C2158.7244,-654.0105 2121.0534,-641.5535 2088.2735,-630.7139\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2089.3128,-627.3713 2078.7195,-627.5547 2087.115,-634.0173 2089.3128,-627.3713\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872718656&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge30\">\n<title>140252872718656-&gt;140252872260352</title>\n<path d=\"M2553.2475,-664.4901C2452.0493,-653.3354 2330.527,-639.9406 2228.0088,-628.6404\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2228.1553,-625.1355 2217.832,-627.5187 2227.3884,-632.0933 2228.1553,-625.1355\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872027384&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge31\">\n<title>140252872027384-&gt;140252872260352</title>\n<path d=\"M3011.8996,-664.6744C3008.9164,-664.4459 3005.949,-664.221 3003,-664 2813.9231,-649.8321 2604.4106,-636.8829 2426.6772,-626.7086\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2426.8146,-623.2109 2416.6312,-626.1346 2426.4152,-630.1995 2426.8146,-623.2109\" stroke=\"#000000\"/>\n</g>\n<!-- 140252872257888&#45;&gt;140252872260352 -->\n<g class=\"edge\" id=\"edge32\">\n<title>140252872257888-&gt;140252872260352</title>\n<path d=\"M3533.8632,-664.621C3530.8914,-664.4078 3527.9359,-664.2006 3525,-664 3154.2504,-638.6698 2731.7023,-623.1876 2427.0557,-614.392\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2426.8175,-610.8838 2416.721,-614.0949 2426.6163,-617.8809 2426.8175,-610.8838\" stroke=\"#000000\"/>\n</g>\n<!-- 140252871885096 -->\n<g class=\"node\" id=\"node27\">\n<title>140252871885096</title>\n<polygon fill=\"none\" points=\"1801,-498.5 1801,-544.5 2217,-544.5 2217,-498.5 1801,-498.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1936.5\" y=\"-517.8\">batch_normalization_3: BatchNormalization</text>\n<polyline fill=\"none\" points=\"2072,-498.5 2072,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2101\" y=\"-529.3\">input:</text>\n<polyline fill=\"none\" points=\"2072,-521.5 2130,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2101\" y=\"-506.3\">output:</text>\n<polyline fill=\"none\" points=\"2130,-498.5 2130,-544.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2173.5\" y=\"-529.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"2130,-521.5 2217,-521.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2173.5\" y=\"-506.3\">(None, 256)</text>\n</g>\n<!-- 140252872260352&#45;&gt;140252871885096 -->\n<g class=\"edge\" id=\"edge33\">\n<title>140252872260352-&gt;140252871885096</title>\n<path d=\"M2009,-581.3799C2009,-573.1745 2009,-563.7679 2009,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-554.784 2009,-544.784 2005.5001,-554.784 2012.5001,-554.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252871974360 -->\n<g class=\"node\" id=\"node28\">\n<title>140252871974360</title>\n<polygon fill=\"none\" points=\"1883,-415.5 1883,-461.5 2135,-461.5 2135,-415.5 1883,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1936.5\" y=\"-434.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"1990,-415.5 1990,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2019\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"1990,-438.5 2048,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2019\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"2048,-415.5 2048,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-446.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"2048,-438.5 2135,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-423.3\">(None, 64)</text>\n</g>\n<!-- 140252871885096&#45;&gt;140252871974360 -->\n<g class=\"edge\" id=\"edge34\">\n<title>140252871885096-&gt;140252871974360</title>\n<path d=\"M2009,-498.3799C2009,-490.1745 2009,-480.7679 2009,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-471.784 2009,-461.784 2005.5001,-471.784 2012.5001,-471.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252871887504 -->\n<g class=\"node\" id=\"node29\">\n<title>140252871887504</title>\n<polygon fill=\"none\" points=\"1873,-332.5 1873,-378.5 2145,-378.5 2145,-332.5 1873,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1940\" y=\"-351.8\">dropout_3: Dropout</text>\n<polyline fill=\"none\" points=\"2007,-332.5 2007,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2036\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"2007,-355.5 2065,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2036\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"2065,-332.5 2065,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2105\" y=\"-363.3\">(None, 64)</text>\n<polyline fill=\"none\" points=\"2065,-355.5 2145,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2105\" y=\"-340.3\">(None, 64)</text>\n</g>\n<!-- 140252871974360&#45;&gt;140252871887504 -->\n<g class=\"edge\" id=\"edge35\">\n<title>140252871974360-&gt;140252871887504</title>\n<path d=\"M2009,-415.3799C2009,-407.1745 2009,-397.7679 2009,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-388.784 2009,-378.784 2005.5001,-388.784 2012.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252871587880 -->\n<g class=\"node\" id=\"node30\">\n<title>140252871587880</title>\n<polygon fill=\"none\" points=\"1804.5,-249.5 1804.5,-295.5 2213.5,-295.5 2213.5,-249.5 1804.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1940\" y=\"-268.8\">batch_normalization_4: BatchNormalization</text>\n<polyline fill=\"none\" points=\"2075.5,-249.5 2075.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2104.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"2075.5,-272.5 2133.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2104.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"2133.5,-249.5 2133.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2173.5\" y=\"-280.3\">(None, 64)</text>\n<polyline fill=\"none\" points=\"2133.5,-272.5 2213.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2173.5\" y=\"-257.3\">(None, 64)</text>\n</g>\n<!-- 140252871887504&#45;&gt;140252871587880 -->\n<g class=\"edge\" id=\"edge36\">\n<title>140252871887504-&gt;140252871587880</title>\n<path d=\"M2009,-332.3799C2009,-324.1745 2009,-314.7679 2009,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-305.784 2009,-295.784 2005.5001,-305.784 2012.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252870904408 -->\n<g class=\"node\" id=\"node31\">\n<title>140252870904408</title>\n<polygon fill=\"none\" points=\"1886.5,-166.5 1886.5,-212.5 2131.5,-212.5 2131.5,-166.5 1886.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1940\" y=\"-185.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"1993.5,-166.5 1993.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2022.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"1993.5,-189.5 2051.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2022.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"2051.5,-166.5 2051.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-197.3\">(None, 64)</text>\n<polyline fill=\"none\" points=\"2051.5,-189.5 2131.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-174.3\">(None, 32)</text>\n</g>\n<!-- 140252871587880&#45;&gt;140252870904408 -->\n<g class=\"edge\" id=\"edge37\">\n<title>140252871587880-&gt;140252870904408</title>\n<path d=\"M2009,-249.3799C2009,-241.1745 2009,-231.7679 2009,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-222.784 2009,-212.784 2005.5001,-222.784 2012.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252870753864 -->\n<g class=\"node\" id=\"node32\">\n<title>140252870753864</title>\n<polygon fill=\"none\" points=\"1873,-83.5 1873,-129.5 2145,-129.5 2145,-83.5 1873,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1940\" y=\"-102.8\">dropout_4: Dropout</text>\n<polyline fill=\"none\" points=\"2007,-83.5 2007,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2036\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"2007,-106.5 2065,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2036\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"2065,-83.5 2065,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2105\" y=\"-114.3\">(None, 32)</text>\n<polyline fill=\"none\" points=\"2065,-106.5 2145,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2105\" y=\"-91.3\">(None, 32)</text>\n</g>\n<!-- 140252870904408&#45;&gt;140252870753864 -->\n<g class=\"edge\" id=\"edge38\">\n<title>140252870904408-&gt;140252870753864</title>\n<path d=\"M2009,-166.3799C2009,-158.1745 2009,-148.7679 2009,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-139.784 2009,-129.784 2005.5001,-139.784 2012.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140252870690296 -->\n<g class=\"node\" id=\"node33\">\n<title>140252870690296</title>\n<polygon fill=\"none\" points=\"1886.5,-.5 1886.5,-46.5 2131.5,-46.5 2131.5,-.5 1886.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1940\" y=\"-19.8\">dense_6: Dense</text>\n<polyline fill=\"none\" points=\"1993.5,-.5 1993.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2022.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"1993.5,-23.5 2051.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2022.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"2051.5,-.5 2051.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-31.3\">(None, 32)</text>\n<polyline fill=\"none\" points=\"2051.5,-23.5 2131.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"2091.5\" y=\"-8.3\">(None, 5)</text>\n</g>\n<!-- 140252870753864&#45;&gt;140252870690296 -->\n<g class=\"edge\" id=\"edge39\">\n<title>140252870753864-&gt;140252870690296</title>\n<path d=\"M2009,-83.3799C2009,-75.1745 2009,-65.7679 2009,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"2012.5001,-56.784 2009,-46.784 2005.5001,-56.784 2012.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "bHYz1kXFLhIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_preds = np.zeros((test.shape[0], 5))\n",
        "file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                              save_best_only = True, mode = \"min\")\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 2)\n",
        "\n",
        "print(\"Building the model...\")\n",
        "model3 = build_model3(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    print(\"FOLD\", i+1)    \n",
        "    print(\"Splitting the data into train and validation...\")\n",
        "    train_seq, val_seq = X_train[train[\"fold_id\"] != i], X_train[train[\"fold_id\"] == i]\n",
        "    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
        "    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))        \n",
        "    \n",
        "    print(\"Training the model...\")\n",
        "    model3.fit(train_seq, y_train, validation_data = (val_seq, y_val), batch_size = 128, epochs = 15, verbose = 1, callbacks = [early_stop]) \n",
        "    model3.save_weights(file_path)  \n",
        "    test_preds += model2.predict([X_test], batch_size=1024, verbose=1)    \n",
        "    print()\n",
        "    \n",
        "print(\"Save model after cross-validation...\")\n",
        "model3.save_weights(file_path)   \n",
        "test_preds /= NUM_FOLDS\n",
        "\n",
        "\n",
        "print(\"Make the submission ready...\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "\n",
        "pred = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/drive/My Drive/DeepLearning/blend.csv\", index=False)\n",
        "\n",
        "predictions = np.round(np.argmax(test_preds, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/drive/My Drive/DeepLearning/avg_blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5u0R1sLfthlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model4(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, ktop=5):\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    x_conv1 = LeakyReLU(0.2)(x_conv1)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    #dyn_pool1_gru = DynamicKMaxPoolLayer(x_conv1,ktop,nroflayers=2,layernr=1)\n",
        "    \n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    x_conv2 = LeakyReLU(0.2)(x_conv2)\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    #dyn_pool2_gru = DynamicKMaxPoolLayer(x_conv2,ktop,nroflayers=2,layernr=1)\n",
        "\n",
        "    \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    x_conv3 = LeakyReLU(0.2)(x_conv3)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    #dyn_pool1_lstm = DynamicKMaxPoolLayer(x_conv3,ktop,nroflayers=2,layernr=1)\n",
        "    \n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    x_conv4 = LeakyReLU(0.2)(x_conv4)\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    #dyn_pool2_lstm = DynamicKMaxPoolLayer(x_conv4,ktop,nroflayers=2,layernr=1)\n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L3kxtJabtkYT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model4 = build_model4(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32, ktop=5)\n",
        "\n",
        "model4.summary()\n",
        "SVG(model_to_dot(model4,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sjHlbhfUtkci",
        "colab_type": "code",
        "outputId": "e04e3c21-4ae2-41a4-d908-220341c3b8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3313
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_FOLDS = 20\n",
        "train[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n",
        "test_preds = np.zeros((test.shape[0], 5))\n",
        "file_path = \"/content/drive/My Drive/DeepLearning/best_model.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                              save_best_only = True, mode = \"min\")\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 2)\n",
        "\n",
        "print(\"Building the model...\")\n",
        "model4 = build_model4(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=4, dense_units=64, dr=0.3, conv_size=32)\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    print(\"FOLD\", i+1)    \n",
        "    print(\"Splitting the data into train and validation...\")\n",
        "    train_seq, val_seq = X_train[train[\"fold_id\"] != i], X_train[train[\"fold_id\"] == i]\n",
        "    y_train = ohe.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
        "    y_val = ohe.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))        \n",
        "    \n",
        "    print(\"Training the model...\")\n",
        "    model4.fit(train_seq, y_train, validation_data = (val_seq, y_val), batch_size = 128, epochs = 15, verbose = 1, callbacks = [early_stop]) \n",
        "    model4.save_weights(file_path)  \n",
        "    test_preds += model4.predict([X_test], batch_size=1024, verbose=1)    \n",
        "    print()\n",
        "    \n",
        "print(\"Save model after cross-validation...\")\n",
        "model4.save_weights(file_path)   \n",
        "test_preds /= NUM_FOLDS\n",
        "\n",
        "\n",
        "print(\"Make the submission ready...\")\n",
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "\n",
        "pred = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/blend.csv\", index=False)\n",
        "\n",
        "predictions = np.round(np.argmax(test_preds, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/avg_blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the model...\n",
            "FOLD 1\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148193 samples, validate on 7867 samples\n",
            "Epoch 1/15\n",
            "148193/148193 [==============================] - 82s 553us/step - loss: 0.3688 - acc: 0.8348 - val_loss: 0.3077 - val_acc: 0.8578\n",
            "Epoch 2/15\n",
            "148193/148193 [==============================] - 72s 485us/step - loss: 0.3255 - acc: 0.8525 - val_loss: 0.3013 - val_acc: 0.8608\n",
            "Epoch 3/15\n",
            "148193/148193 [==============================] - 72s 485us/step - loss: 0.3152 - acc: 0.8566 - val_loss: 0.3010 - val_acc: 0.8609\n",
            "Epoch 4/15\n",
            "148193/148193 [==============================] - 72s 486us/step - loss: 0.3069 - acc: 0.8599 - val_loss: 0.3034 - val_acc: 0.8624\n",
            "Epoch 5/15\n",
            "148193/148193 [==============================] - 72s 487us/step - loss: 0.3018 - acc: 0.8617 - val_loss: 0.2919 - val_acc: 0.8679\n",
            "Epoch 6/15\n",
            "148193/148193 [==============================] - 71s 476us/step - loss: 0.2963 - acc: 0.8638 - val_loss: 0.2904 - val_acc: 0.8672\n",
            "Epoch 7/15\n",
            "148193/148193 [==============================] - 71s 479us/step - loss: 0.2922 - acc: 0.8662 - val_loss: 0.2911 - val_acc: 0.8676\n",
            "Epoch 8/15\n",
            "148193/148193 [==============================] - 73s 490us/step - loss: 0.2885 - acc: 0.8681 - val_loss: 0.2927 - val_acc: 0.8675\n",
            "66292/66292 [==============================] - 4s 59us/step\n",
            "\n",
            "FOLD 2\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148316 samples, validate on 7744 samples\n",
            "Epoch 1/15\n",
            "148316/148316 [==============================] - 73s 494us/step - loss: 0.2862 - acc: 0.8692 - val_loss: 0.2620 - val_acc: 0.8816\n",
            "Epoch 2/15\n",
            "148316/148316 [==============================] - 72s 486us/step - loss: 0.2835 - acc: 0.8705 - val_loss: 0.2649 - val_acc: 0.8789\n",
            "Epoch 3/15\n",
            "148316/148316 [==============================] - 72s 483us/step - loss: 0.2817 - acc: 0.8716 - val_loss: 0.2667 - val_acc: 0.8789\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 3\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148487 samples, validate on 7573 samples\n",
            "Epoch 1/15\n",
            "148487/148487 [==============================] - 73s 494us/step - loss: 0.2793 - acc: 0.8726 - val_loss: 0.2469 - val_acc: 0.8883\n",
            "Epoch 2/15\n",
            "148487/148487 [==============================] - 72s 487us/step - loss: 0.2774 - acc: 0.8739 - val_loss: 0.2498 - val_acc: 0.8864\n",
            "Epoch 3/15\n",
            "148487/148487 [==============================] - 72s 488us/step - loss: 0.2761 - acc: 0.8748 - val_loss: 0.2508 - val_acc: 0.8862\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 4\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148073 samples, validate on 7987 samples\n",
            "Epoch 1/15\n",
            "148073/148073 [==============================] - 73s 494us/step - loss: 0.2748 - acc: 0.8758 - val_loss: 0.2422 - val_acc: 0.8906\n",
            "Epoch 2/15\n",
            "148073/148073 [==============================] - 72s 487us/step - loss: 0.2727 - acc: 0.8764 - val_loss: 0.2441 - val_acc: 0.8889\n",
            "Epoch 3/15\n",
            "148073/148073 [==============================] - 72s 487us/step - loss: 0.2710 - acc: 0.8777 - val_loss: 0.2463 - val_acc: 0.8875\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 5\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148054 samples, validate on 8006 samples\n",
            "Epoch 1/15\n",
            "148054/148054 [==============================] - 73s 496us/step - loss: 0.2698 - acc: 0.8782 - val_loss: 0.2416 - val_acc: 0.8935\n",
            "Epoch 2/15\n",
            "148054/148054 [==============================] - 72s 486us/step - loss: 0.2683 - acc: 0.8788 - val_loss: 0.2398 - val_acc: 0.8924\n",
            "Epoch 3/15\n",
            "148054/148054 [==============================] - 74s 502us/step - loss: 0.2668 - acc: 0.8794 - val_loss: 0.2445 - val_acc: 0.8901\n",
            "Epoch 4/15\n",
            "148054/148054 [==============================] - 73s 492us/step - loss: 0.2663 - acc: 0.8800 - val_loss: 0.2467 - val_acc: 0.8892\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 6\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148575 samples, validate on 7485 samples\n",
            "Epoch 1/15\n",
            "148575/148575 [==============================] - 74s 496us/step - loss: 0.2655 - acc: 0.8804 - val_loss: 0.2380 - val_acc: 0.8942\n",
            "Epoch 2/15\n",
            "148575/148575 [==============================] - 74s 499us/step - loss: 0.2644 - acc: 0.8812 - val_loss: 0.2420 - val_acc: 0.8915\n",
            "Epoch 3/15\n",
            "148575/148575 [==============================] - 72s 487us/step - loss: 0.2630 - acc: 0.8818 - val_loss: 0.2433 - val_acc: 0.8915\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 7\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148126 samples, validate on 7934 samples\n",
            "Epoch 1/15\n",
            "148126/148126 [==============================] - 74s 497us/step - loss: 0.2630 - acc: 0.8820 - val_loss: 0.2343 - val_acc: 0.8967\n",
            "Epoch 2/15\n",
            "148126/148126 [==============================] - 74s 500us/step - loss: 0.2615 - acc: 0.8823 - val_loss: 0.2370 - val_acc: 0.8955\n",
            "Epoch 3/15\n",
            "148126/148126 [==============================] - 72s 488us/step - loss: 0.2610 - acc: 0.8834 - val_loss: 0.2386 - val_acc: 0.8938\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 8\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148199 samples, validate on 7861 samples\n",
            "Epoch 1/15\n",
            "148199/148199 [==============================] - 73s 495us/step - loss: 0.2612 - acc: 0.8826 - val_loss: 0.2253 - val_acc: 0.9020\n",
            "Epoch 2/15\n",
            "148199/148199 [==============================] - 74s 499us/step - loss: 0.2593 - acc: 0.8839 - val_loss: 0.2315 - val_acc: 0.8985\n",
            "Epoch 3/15\n",
            "148199/148199 [==============================] - 74s 502us/step - loss: 0.2590 - acc: 0.8841 - val_loss: 0.2306 - val_acc: 0.8986\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 9\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148078 samples, validate on 7982 samples\n",
            "Epoch 1/15\n",
            "148078/148078 [==============================] - 74s 499us/step - loss: 0.2591 - acc: 0.8842 - val_loss: 0.2253 - val_acc: 0.9005\n",
            "Epoch 2/15\n",
            "148078/148078 [==============================] - 72s 487us/step - loss: 0.2579 - acc: 0.8844 - val_loss: 0.2246 - val_acc: 0.9013\n",
            "Epoch 3/15\n",
            "148078/148078 [==============================] - 72s 488us/step - loss: 0.2569 - acc: 0.8852 - val_loss: 0.2300 - val_acc: 0.8980\n",
            "Epoch 4/15\n",
            "148078/148078 [==============================] - 73s 494us/step - loss: 0.2569 - acc: 0.8850 - val_loss: 0.2293 - val_acc: 0.8986\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 10\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148142 samples, validate on 7918 samples\n",
            "Epoch 1/15\n",
            "148142/148142 [==============================] - 73s 492us/step - loss: 0.2570 - acc: 0.8853 - val_loss: 0.2174 - val_acc: 0.9064\n",
            "Epoch 2/15\n",
            "148142/148142 [==============================] - 72s 487us/step - loss: 0.2560 - acc: 0.8857 - val_loss: 0.2184 - val_acc: 0.9055\n",
            "Epoch 3/15\n",
            "148142/148142 [==============================] - 73s 492us/step - loss: 0.2556 - acc: 0.8860 - val_loss: 0.2177 - val_acc: 0.9063\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 11\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148164 samples, validate on 7896 samples\n",
            "Epoch 1/15\n",
            "148164/148164 [==============================] - 74s 496us/step - loss: 0.2551 - acc: 0.8861 - val_loss: 0.2121 - val_acc: 0.9096\n",
            "Epoch 2/15\n",
            "148164/148164 [==============================] - 73s 490us/step - loss: 0.2543 - acc: 0.8863 - val_loss: 0.2162 - val_acc: 0.9085\n",
            "Epoch 3/15\n",
            "148164/148164 [==============================] - 72s 485us/step - loss: 0.2536 - acc: 0.8868 - val_loss: 0.2170 - val_acc: 0.9074\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 12\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148072 samples, validate on 7988 samples\n",
            "Epoch 1/15\n",
            "148072/148072 [==============================] - 73s 494us/step - loss: 0.2537 - acc: 0.8865 - val_loss: 0.2132 - val_acc: 0.9088\n",
            "Epoch 2/15\n",
            "148072/148072 [==============================] - 72s 486us/step - loss: 0.2534 - acc: 0.8875 - val_loss: 0.2157 - val_acc: 0.9084\n",
            "Epoch 3/15\n",
            "148072/148072 [==============================] - 73s 496us/step - loss: 0.2522 - acc: 0.8877 - val_loss: 0.2175 - val_acc: 0.9065\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 13\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148135 samples, validate on 7925 samples\n",
            "Epoch 1/15\n",
            "148135/148135 [==============================] - 71s 480us/step - loss: 0.2536 - acc: 0.8876 - val_loss: 0.2040 - val_acc: 0.9124\n",
            "Epoch 2/15\n",
            "148135/148135 [==============================] - 70s 473us/step - loss: 0.2534 - acc: 0.8872 - val_loss: 0.2070 - val_acc: 0.9097\n",
            "Epoch 3/15\n",
            "148135/148135 [==============================] - 71s 479us/step - loss: 0.2521 - acc: 0.8878 - val_loss: 0.2117 - val_acc: 0.9092\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 14\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148300 samples, validate on 7760 samples\n",
            "Epoch 1/15\n",
            "148300/148300 [==============================] - 71s 476us/step - loss: 0.2510 - acc: 0.8883 - val_loss: 0.2099 - val_acc: 0.9081\n",
            "Epoch 2/15\n",
            "148300/148300 [==============================] - 71s 480us/step - loss: 0.2511 - acc: 0.8882 - val_loss: 0.2124 - val_acc: 0.9064\n",
            "Epoch 3/15\n",
            "148300/148300 [==============================] - 71s 476us/step - loss: 0.2505 - acc: 0.8886 - val_loss: 0.2164 - val_acc: 0.9053\n",
            "66292/66292 [==============================] - 3s 50us/step\n",
            "\n",
            "FOLD 15\n",
            "Splitting the data into train and validation...\n",
            "Training the model...\n",
            "Train on 148443 samples, validate on 7617 samples\n",
            "Epoch 1/15\n",
            " 79744/148443 [===============>..............] - ETA: 31s - loss: 0.2501 - acc: 0.8893Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tLziOKQltkg9",
        "colab_type": "code",
        "outputId": "fe22bca9-dbb1-4534-d238-1ab8a9930219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv('/content/drive/My Drive/DeepLearning/sampleSubmission.csv', sep=\",\")\n",
        "\n",
        "pred = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/blend20.csv\", index=False)\n",
        "\n",
        "predictions = np.round(np.argmax(test_preds, axis=1)).astype(int)\n",
        "submission = pd.DataFrame({'PhraseId':test['PhraseId'],'Sentiment': predictions})\n",
        "submission =pd.merge(submission, save_test, on='PhraseId', how='left')\n",
        "submission[\"Sentiment\"] = submission.apply(get_sentiment, axis=1)\n",
        "submission.drop(['Sentiment_x', 'Sentiment_y'], axis=1,inplace=True)\n",
        "submission[\"Sentiment\"] = submission[\"Sentiment\"].astype(int)\n",
        "submission.to_csv(\"/content/drive/My Drive/DeepLearning/avg_blend20.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66292/66292 [==============================] - 3s 51us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2JGcOSPBIP7",
        "colab_type": "code",
        "outputId": "e7f10c4d-3970-430c-f1e6-184a48f47736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install theano\n",
        "!pip install lasagne"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BoTiBvritkn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/My Drive/DeepLearning/avg_blend20.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onIcjzteA77o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import theano.tensor as T\n",
        "from lasagne.layers.base import Layer\n",
        "\n",
        "\n",
        "class KMaxPoolLayer(Layer):\n",
        "\n",
        "    def __init__(self,incoming,k,**kwargs):\n",
        "        super(KMaxPoolLayer, self).__init__(incoming, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2], self.k)\n",
        "\n",
        "    def get_output_for(self, input, **kwargs):\n",
        "        return self.kmaxpooling(input,self.k)\n",
        "\n",
        "\n",
        "    def kmaxpooling(self,input,k):\n",
        "\n",
        "        sorted_values = T.argsort(input,axis=3)\n",
        "        topmax_indexes = sorted_values[:,:,:,-k:]\n",
        "        # sort indexes so that we keep the correct order within the sentence\n",
        "        topmax_indexes_sorted = T.sort(topmax_indexes)\n",
        "\n",
        "        #given that topmax only gives the index of the third dimension, we need to generate the other 3 dimensions\n",
        "        dim0 = T.arange(0,self.input_shape[0]).repeat(self.input_shape[1]*self.input_shape[2]*k)\n",
        "        dim1 = T.arange(0,self.input_shape[1]).repeat(k*self.input_shape[2]).reshape((1,-1)).repeat(self.input_shape[0],axis=0).flatten()\n",
        "        dim2 = T.arange(0,self.input_shape[2]).repeat(k).reshape((1,-1)).repeat(self.input_shape[0]*self.input_shape[1],axis=0).flatten()\n",
        "        dim3 = topmax_indexes_sorted.flatten()\n",
        "        return input[dim0,dim1,dim2,dim3].reshape((self.input_shape[0], self.input_shape[1], self.input_shape[2], k))\n",
        "\n",
        "\n",
        "\n",
        "class DynamicKMaxPoolLayer(KMaxPoolLayer):\n",
        "\n",
        "    def __init__(self,incoming,ktop,nroflayers,layernr,**kwargs):\n",
        "        super(DynamicKMaxPoolLayer, self).__init__(incoming,ktop, **kwargs)\n",
        "        self.ktop = ktop\n",
        "        self.layernr = layernr\n",
        "        self.nroflayers = nroflayers\n",
        "\n",
        "    def get_k(self,input_shape):\n",
        "        return T.cast(T.max([self.ktop,T.ceil((self.nroflayers-self.layernr)/float(self.nroflayers)*input_shape[3])]),'int32')\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2], None)\n",
        "\n",
        "    def get_output_for(self, input, **kwargs):\n",
        "\n",
        "        k = self.get_k(input.shape)\n",
        "\n",
        "        return self.kmaxpooling(input,k)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}